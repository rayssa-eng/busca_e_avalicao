{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rayss\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import xml.etree.ElementTree as ET\n",
    "import unicodedata\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo arquivo de configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('GLI.cfg')\n",
    "\n",
    "xml_paths = [path.strip() for path in config.get('DEFAULT', 'LEIA').split(',')]\n",
    "li_path = config.get('DEFAULT', 'ESCREVA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abrindo arquivos recuperados do arquivo de configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    fs = [open(path, \"r\") for path in xml_paths]\n",
    "except:\n",
    "  print(\"Something went wrong when opening one or more files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtendo campos raíz de cada arquivo e, subsequentemente, os dados dos campos RECORDNUM e ABSTRACT/EXTRACT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_trees = [ET.parse(f) for f in fs]\n",
    "doc_roots = [doc_tree.getroot() for doc_tree in doc_trees]\n",
    "\n",
    "def get_doc_data(doc_roots):\n",
    "    doc_data = {}\n",
    "    for doc_root in doc_roots:\n",
    "        for rec_element in doc_root.findall('RECORD'):\n",
    "            rec_num = rec_element.find('RECORDNUM').text\n",
    "            abstract_element = rec_element.find('ABSTRACT')\n",
    "            if abstract_element is not None:\n",
    "                abstract_content = abstract_element.text\n",
    "            else:\n",
    "                abstract_element = rec_element.find('EXTRACT')\n",
    "                abstract_content = abstract_element.text if abstract_element is not None else \"None\"\n",
    "            doc_data[rec_num] = abstract_content\n",
    "    return doc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colocando todos os dados de todos os arquivos num dicionário, onde a chave é RECORDNUM e o valor é o conteúdo ABSTRACT/EXTRACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_data = get_doc_data(doc_roots)\n",
    "no_of_docs = len(all_files_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processando os conteúdos dos arquivos no dicionário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_MIN_LENGTH = 2\n",
    "STOP_WORDS_ENG = [stop_word.lower() for stop_word in nltk.corpus.stopwords.words('english')]\n",
    "\n",
    "def strip_accents(text):\n",
    "    nfkd = unicodedata.normalize('NFKD', text)\n",
    "    stripped_text = u\"\".join([c for c in nfkd if not unicodedata.combining(c)])\n",
    "    return re.sub('[^a-zA-Z]', ' ', stripped_text)\n",
    "\n",
    "def tokenize(text):\n",
    "    text = strip_accents(text)\n",
    "    text = re.sub(re.compile('\\n'),' ',text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in STOP_WORDS_ENG and len(word) >= WORD_MIN_LENGTH]\n",
    "    return words\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatized_text = []\n",
    "    for (word) in text:\n",
    "        lemmatizing = str(WordNetLemmatizer().lemmatize(word))\n",
    "        lemmatized_text.append(lemmatizing)\n",
    "    return lemmatized_text\n",
    "\n",
    "def preprocess(data):\n",
    "    preprocessed_data = {}\n",
    "    for rec_num, text in data.items():\n",
    "        preprocessed_data[rec_num] = tokenize(text)\n",
    "        preprocessed_data[rec_num] = lemmatize(preprocessed_data[rec_num])\n",
    "        preprocessed_data[rec_num] = [word.upper() for word in preprocessed_data[rec_num]]\n",
    "    return preprocessed_data\n",
    "\n",
    "def get_unique_words(preprocessed_data):\n",
    "    all_words = []\n",
    "    for rec_num, words in preprocessed_data.items():\n",
    "        for word in words:\n",
    "            all_words.append(word)\n",
    "    \n",
    "    unique_words = nltk.FreqDist(all_words).keys()\n",
    "    unique_words = [word.upper() for word in unique_words]\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Textos pré-processados e lista de palavras únicas entre todos os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files_data_preprocessed = preprocess(all_files_data)\n",
    "unique_words_list = get_unique_words(all_files_data_preprocessed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculando o número de termos em cada documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'n_words' (dict)\n"
     ]
    }
   ],
   "source": [
    "n_words = {}\n",
    "for rec_num, words in all_files_data_preprocessed.items():\n",
    "    n_words[rec_num] = len(words)\n",
    "\n",
    "n_words\n",
    "%store n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando a base de dados pré-processada para posterior uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./base_preprocessada.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    \n",
    "    # Write rows\n",
    "    for key, value in all_files_data_preprocessed.items():\n",
    "        writer.writerow([key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gli(words, preprocessed_data):\n",
    "    inverted_index = {}\n",
    "    for rec_num, text in preprocessed_data.items():\n",
    "        for word in words:\n",
    "            if word in text:\n",
    "                if word not in inverted_index:\n",
    "                    inverted_index[word] = []\n",
    "                inverted_index[word].extend([rec_num] * text.count(word))\n",
    "    return inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'li' (dict)\n"
     ]
    }
   ],
   "source": [
    "li = gli(unique_words_list, all_files_data_preprocessed)\n",
    "li\n",
    "%store li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escrevendo arquivo .csv para a Lista Invertida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(li_path, 'w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=';')\n",
    "    \n",
    "    # Write rows\n",
    "    for key, value in li.items():\n",
    "        writer.writerow([key, value])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
